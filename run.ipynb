{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f1364f",
   "metadata": {},
   "source": [
    "### Please set the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99534c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (11, 11)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "environment = \"maze\" # among random maze, cliff, hole or taxi\n",
    "\n",
    "if environment == \"taxi\":  # select taxi environment from gym\n",
    "    from taxi.training_utils import *\n",
    "    from taxi.learning_algorithms import *\n",
    "    env = gym.make(\"Taxi-v3\").env\n",
    "elif environment == \"maze\":  # select random maze\n",
    "    from mazes.training_utils import *\n",
    "    from mazes.learning_algorithms import *\n",
    "    from environments.random_maze import *\n",
    "    env_id = \"RandomMaze-v0\"\n",
    "    gym.envs.register(id=env_id, entry_point=RandomMaze, max_episode_steps=100)\n",
    "    env = gym.make(env_id)\n",
    "elif environment == \"cliff\":  # select cliff\n",
    "    from mazes.training_utils import *\n",
    "    from mazes.learning_algorithms import *\n",
    "    from environments.cliff import *\n",
    "    env_id = \"RandomCliff-v0\"\n",
    "    gym.envs.register(id=env_id, entry_point=RandomCliff, max_episode_steps=100)\n",
    "    env = gym.make(env_id)\n",
    "elif environment == \"hole\":  # select hole environment\n",
    "    from mazes.training_utils import *\n",
    "    from mazes.learning_algorithms import *\n",
    "    from environments.hole import *\n",
    "    env_id = \"Hole-v0\"\n",
    "    gym.envs.register(id=env_id, entry_point=RandomHole, max_episode_steps=100)\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "import pickle\n",
    "from utils.utils import *\n",
    "\n",
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f118ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "test_freq = 50\n",
    "num_episodes = 10000\n",
    "num_avg = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fabe0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TRAINING RUN 0 OUT OF 10 ===========\n",
      "********** TRAINING WITH SCRN **********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be uint8, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be uint8, actual type: int32\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Federico Betti\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomMaze' object has no attribute 'compute_optimal_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m========== TRAINING RUN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m OUT OF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_avg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m********** TRAINING WITH SCRN **********\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m stats_SCRN \u001b[38;5;241m=\u001b[39m \u001b[43mdiscrete_SCRN\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCRN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate({i: stats_SCRN})\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m********** TRAINING WITH SPG ********\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Global-Convergence-in-RL\\mazes\\learning_algorithms.py:128\u001b[0m, in \u001b[0;36mdiscrete_SCRN\u001b[1;34m(env, num_episodes, alpha, gamma, batch_size, SGD, period, test_freq, step_cache, reward_cache, env_cache, name_cache)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Hessian \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((STATE_DIM \u001b[38;5;241m*\u001b[39m ACTION_DIM, STATE_DIM \u001b[38;5;241m*\u001b[39m ACTION_DIM))\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m test_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     optimum, estimate_obj, estimate_grad, sample_traj \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_objective_and_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m                                                                                        \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     tau_estimates\u001b[38;5;241m.\u001b[39mappend((optimum \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(estimate_obj)) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(estimate_grad))\n\u001b[0;32m    131\u001b[0m     estimates[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_traj\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(sample_traj)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Global-Convergence-in-RL\\mazes\\training_utils.py:244\u001b[0m, in \u001b[0;36mestimate_objective_and_gradient\u001b[1;34m(env, gamma, theta, num_episodes)\u001b[0m\n\u001b[0;32m    241\u001b[0m obj \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    242\u001b[0m grad \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 244\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_optimal_actions\u001b[49m()\n\u001b[0;32m    246\u001b[0m env\u001b[38;5;241m.\u001b[39mreset_position()\n\u001b[0;32m    247\u001b[0m reward_trajectory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomMaze' object has no attribute 'compute_optimal_actions'"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    stats = {\"SCRN\": {}, \"SPG\": {}, \"SPG Entropy\": {}}\n",
    "    for i in range(num_avg):\n",
    "        print(f\"========== TRAINING RUN {i} OUT OF {num_avg} ===========\")\n",
    "        print(\"********** TRAINING WITH SCRN **********\")\n",
    "        stats_SCRN = discrete_SCRN(env, num_episodes=num_episodes, test_freq=test_freq)\n",
    "        stats[\"SCRN\"].update({i: stats_SCRN})\n",
    "        print(\"********** TRAINING WITH SPG ********\")\n",
    "        stats_DPG = discrete_policy_gradient(env, num_episodes=num_episodes, test_freq=test_freq)\n",
    "        stats[\"SPG\"].update({i: stats_DPG})\n",
    "        print(\"********** TRAINING WITH regularized SPG ********\")\n",
    "        stats_DPG = discrete_policy_gradient(env, entropy_bonus=True, num_episodes=num_episodes, test_freq=test_freq)\n",
    "        stats[\"SPG Entropy\"].update({i: stats_DPG})\n",
    "else:\n",
    "    with open(\"results.pkl\", \"rb\") as f:\n",
    "        average_stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec11d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    print(i, stats_SCRN[\"theta\"][0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_stats = {\"SCRN\": {}, \"SPG\": {}, \"SPG Entropy\": {}}\n",
    "std_stats = {\"SCRN\": {}, \"SPG\": {}, \"SPG Entropy\": {}}\n",
    "average_stats[\"SCRN\"] = {key: np.mean([stats[\"SCRN\"][i][key] for i in range(num_avg)], axis=0)\n",
    "                         for key in [\"good_policy\", \"taus\", \"steps\", \"rewards\", \"history_probs\"]}\n",
    "average_stats[\"SPG\"] = {key: np.mean([stats[\"SPG\"][i][key] for i in range(num_avg)], axis=0)\n",
    "                         for key in [\"good_policy\", \"taus\", \"steps\", \"rewards\", \"history_probs\"]}\n",
    "average_stats[\"SPG Entropy\"] = {key: np.mean([stats[\"SPG Entropy\"][i][key] for i in range(num_avg)], axis=0)\n",
    "                         for key in [\"good_policy\", \"taus\", \"steps\", \"rewards\", \"history_probs\"]}\n",
    "std_stats[\"SCRN\"] = {key: np.std([stats[\"SCRN\"][i][key] for i in range(num_avg)], axis=0) / np.sqrt(num_avg)\n",
    "                         for key in [\"taus\", \"steps\", \"rewards\"]}\n",
    "std_stats[\"SPG\"] = {key: np.median([stats[\"SPG\"][i][key] for i in range(num_avg)], axis=0) / np.sqrt(num_avg)\n",
    "                         for key in [\"taus\", \"steps\", \"rewards\"]}\n",
    "std_stats[\"SPG Entropy\"] = {key: np.median([stats[\"SPG Entropy\"][i][key] for i in range(num_avg)], axis=0) / np.sqrt(num_avg)\n",
    "                         for key in [\"taus\", \"steps\", \"rewards\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_SCRN = True\n",
    "show_SPG = True\n",
    "\n",
    "for key in average_stats.keys():\n",
    "    print(key, average_stats[key][\"good_policy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.figure()\n",
    "QOI_SCRN = running_average(average_stats[\"SCRN\"], \"steps\")\n",
    "STD_SCRN = std_stats[\"SCRN\"][\"steps\"]\n",
    "QOI_SPG = running_average(average_stats[\"SPG\"], \"steps\")\n",
    "STD_SPG = std_stats[\"SPG\"][\"steps\"]\n",
    "QOI_ESPG = running_average(average_stats[\"SPG Entropy\"], \"steps\")\n",
    "STD_ESPG = std_stats[\"SPG Entropy\"][\"steps\"]\n",
    "plt.plot(QOI_SCRN, label=\"SCRN\")\n",
    "plt.fill_between(np.arange(0, num_episodes), QOI_SCRN-STD_SCRN, QOI_SCRN+STD_SCRN, alpha=0.2)\n",
    "plt.plot(QOI_SPG, label=\"SPG\")\n",
    "plt.fill_between(np.arange(0, num_episodes), QOI_SPG-STD_SPG, QOI_SPG+STD_SPG, alpha=0.2)\n",
    "plt.plot(QOI_ESPG, label=\"SPG Entropy\")\n",
    "plt.fill_between(np.arange(0, num_episodes), QOI_ESPG-STD_ESPG, QOI_ESPG+STD_ESPG, alpha=0.2)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Number of episodes\", fontsize=20)\n",
    "plt.ylabel(\"Average episode length\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "QOI_SCRN = average_stats[\"SCRN\"][\"taus\"]\n",
    "STD_SCRN = std_stats[\"SCRN\"][\"taus\"]\n",
    "QOI_SPG = average_stats[\"SPG\"][\"taus\"]\n",
    "STD_SPG = std_stats[\"SPG\"][\"taus\"]\n",
    "QOI_ESPG = average_stats[\"SPG Entropy\"][\"taus\"]\n",
    "STD_ESPG = std_stats[\"SPG Entropy\"][\"taus\"]\n",
    "plt.plot(np.arange(0, num_episodes, step=test_freq), QOI_SCRN, label=\"SCRN\")\n",
    "plt.fill_between(np.arange(0, num_episodes, step=test_freq), QOI_SCRN-STD_SCRN, QOI_SCRN+STD_SCRN, alpha=0.2)\n",
    "plt.plot(np.arange(0, num_episodes, step=test_freq), QOI_SPG, label=\"SPG\")\n",
    "plt.fill_between(np.arange(0, num_episodes, step=test_freq), QOI_SPG-STD_SPG, QOI_SPG+STD_SPG, alpha=0.2)\n",
    "plt.plot(np.arange(0, num_episodes, step=test_freq), QOI_ESPG, label=\"SPG Entropy\")\n",
    "plt.fill_between(np.arange(0, num_episodes, step=test_freq), QOI_ESPG-STD_ESPG, QOI_ESPG+STD_ESPG, alpha=0.2)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.xlabel(\"Number of episodes\", fontsize=20)\n",
    "plt.ylabel(r\"$\\frac{J(\\theta^{*}) - J(\\theta)}{\\vert \\vert \\nabla J(\\theta) \\vert \\vert}$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "QOI_SCRN = running_average(average_stats[\"SCRN\"], \"rewards\")\n",
    "STD_SCRN = std_stats[\"SCRN\"][\"rewards\"]\n",
    "QOI_SPG = running_average(average_stats[\"SPG\"], \"rewards\")\n",
    "STD_SPG = std_stats[\"SPG\"][\"rewards\"]\n",
    "QOI_ESPG = running_average(average_stats[\"SPG Entropy\"], \"rewards\")\n",
    "STD_ESPG = std_stats[\"SPG Entropy\"][\"rewards\"]\n",
    "plt.plot(QOI_SCRN, label=\"SCRN\")\n",
    "plt.fill_between(np.arange(0, num_episodes), QOI_SCRN-STD_SCRN, QOI_SCRN+STD_SCRN, alpha=0.2)\n",
    "plt.plot(QOI_SPG, label=\"SPG\")\n",
    "plt.fill_between(np.arange(0, num_episodes), QOI_SPG-STD_SPG, QOI_SPG+STD_SPG, alpha=0.2)\n",
    "plt.plot(QOI_ESPG, label=\"SPG Entropy\")\n",
    "plt.fill_between(np.arange(0, num_episodes), QOI_ESPG-STD_ESPG, QOI_ESPG+STD_ESPG, alpha=0.2)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Number of episodes\", fontsize=20)\n",
    "plt.ylabel(\"Average reward during training\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4715901",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{environment}_averages.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(average_stats, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f\"{environment}_stds.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(std_stats, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
